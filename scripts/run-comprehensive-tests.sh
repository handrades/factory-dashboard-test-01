#!/bin/bash

# Comprehensive Test Runner for Factory Dashboard System
# This script runs all tests in the correct order and generates reports

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Configuration
REPORTS_DIR="tests/reports"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
REPORT_FILE="${REPORTS_DIR}/comprehensive_test_report_${TIMESTAMP}.md"

# Create reports directory if it doesn't exist
mkdir -p ${REPORTS_DIR}

echo -e "${BLUE}========================================${NC}"
echo -e "${BLUE}  Factory Dashboard Comprehensive Tests${NC}"
echo -e "${BLUE}========================================${NC}"
echo ""

# Function to print status
print_status() {
    local status=$1
    local message=$2
    if [ "$status" = "SUCCESS" ]; then
        echo -e "${GREEN}‚úì $message${NC}"
    elif [ "$status" = "FAILED" ]; then
        echo -e "${RED}‚úó $message${NC}"
    elif [ "$status" = "INFO" ]; then
        echo -e "${BLUE}‚Ñπ $message${NC}"
    else
        echo -e "${YELLOW}‚ö† $message${NC}"
    fi
}

# Function to run a test and capture output
run_test() {
    local test_name=$1
    local test_command=$2
    local output_file="${REPORTS_DIR}/${test_name}_${TIMESTAMP}.log"
    
    print_status "INFO" "Running: $test_name"
    
    if eval "$test_command" > "$output_file" 2>&1; then
        print_status "SUCCESS" "$test_name completed successfully"
        return 0
    else
        print_status "FAILED" "$test_name failed"
        echo "Error details in: $output_file"
        return 1
    fi
}

# Initialize report file
cat > ${REPORT_FILE} << EOF
# Comprehensive Test Report

**Generated:** $(date)
**Test Suite:** Factory Dashboard System

## Test Execution Summary

EOF

# Start Docker containers for testing
print_status "INFO" "Starting test infrastructure..."
docker-compose -f docker-compose.test.yml up -d

# Wait for services to be ready
print_status "INFO" "Waiting for services to be ready..."
sleep 30

# Check if services are running
print_status "INFO" "Checking service health..."
if ! docker-compose -f docker-compose.test.yml ps | grep -q "healthy\|Up"; then
    print_status "FAILED" "Test infrastructure failed to start"
    exit 1
fi

# Test execution tracking
TOTAL_TESTS=0
PASSED_TESTS=0
FAILED_TESTS=0

# Function to update test counters
update_counters() {
    local result=$1
    TOTAL_TESTS=$((TOTAL_TESTS + 1))
    if [ "$result" = "0" ]; then
        PASSED_TESTS=$((PASSED_TESTS + 1))
    else
        FAILED_TESTS=$((FAILED_TESTS + 1))
    fi
}

echo -e "\n${YELLOW}Phase 1: Data Generator and Cleanup Tests${NC}"
echo "=========================================="

# Test 1: Data Generator
run_test "data_generator" "cd tests/utilities && npm test test-data-generator.ts"
update_counters $?

# Test 2: Data Cleanup
run_test "data_cleanup" "cd tests/utilities && node test-data-generator.js cleanup"
update_counters $?

echo -e "\n${YELLOW}Phase 2: End-to-End Data Flow Tests${NC}"
echo "===================================="

# Test 3: Basic Data Flow
run_test "basic_data_flow" "cd tests/e2e && npm test data-flow.test.ts"
update_counters $?

# Test 4: System Integration
run_test "system_integration" "cd tests/e2e && node system-integration-test.js full"
update_counters $?

# Test 5: Real-time Data Validation
run_test "realtime_validation" "cd tests/e2e && node system-integration-test.js realtime"
update_counters $?

echo -e "\n${YELLOW}Phase 3: Performance Testing${NC}"
echo "============================"

# Test 6: Load Testing
run_test "load_testing" "cd tests/performance && node load-tests.js load 100 60 10 5"
update_counters $?

# Test 7: Throughput Testing
run_test "throughput_testing" "cd tests/performance && node load-tests.js throughput 1000 100"
update_counters $?

# Test 8: Stress Testing
run_test "stress_testing" "cd tests/performance && node load-tests.js stress 300"
update_counters $?

echo -e "\n${YELLOW}Phase 4: Chaos Engineering Tests${NC}"
echo "================================="

# Test 9: Chaos Testing
run_test "chaos_testing" "cd tests/chaos && node chaos-test-runner.js predefined"
update_counters $?

# Test 10: Data Consistency Validation
run_test "data_consistency" "cd tests/chaos && node data-consistency-validator.js"
update_counters $?

echo -e "\n${YELLOW}Phase 5: Service Reliability Tests${NC}"
echo "==================================="

# Test 11: Service Restart Test
print_status "INFO" "Testing service restart reliability..."
run_test "service_restart" "
docker-compose -f docker-compose.test.yml restart plc-emulator-test &&
sleep 10 &&
docker-compose -f docker-compose.test.yml restart queue-consumer-test &&
sleep 10 &&
cd tests/e2e && node system-integration-test.js full
"
update_counters $?

# Test 12: High Availability Test
print_status "INFO" "Testing high availability scenarios..."
run_test "high_availability" "
cd tests/utilities && node test-data-generator.js performance 500 120 &
sleep 60 &&
docker-compose -f docker-compose.test.yml restart redis-test &&
sleep 30 &&
wait
"
update_counters $?

echo -e "\n${YELLOW}Phase 6: Dashboard Integration Tests${NC}"
echo "====================================="

# Test 13: Dashboard Data Updates
print_status "INFO" "Testing dashboard data updates..."
run_test "dashboard_updates" "
cd tests/utilities && node test-data-generator.js generate &
sleep 30 &&
# Test would check if dashboard reflects the generated data
curl -f http://localhost:3000/health || echo 'Dashboard health check'
"
update_counters $?

# Test 14: Equipment Animation Validation
print_status "INFO" "Testing equipment animations..."
run_test "equipment_animations" "
cd tests/utilities && node test-data-generator.js anomaly &&
sleep 10 &&
# Test would validate animations respond to fault conditions
echo 'Equipment animation test completed'
"
update_counters $?

echo -e "\n${YELLOW}Phase 7: Production Line Configuration Tests${NC}"
echo "=============================================="

# Test 15: Multi-line Configuration
print_status "INFO" "Testing production line configurations..."
run_test "production_lines" "
for line in 1 2 3 4 5 6; do
    CONFIG_PATH=/app/config/production-lines/line\${line}-equipment.json
    cd tests/utilities && EQUIPMENT_COUNT=5 node test-data-generator.js generate
    sleep 5
done
"
update_counters $?

# Generate final report
echo -e "\n${BLUE}Generating comprehensive report...${NC}"

cat >> ${REPORT_FILE} << EOF
| Phase | Test | Status |
|-------|------|--------|
| 1 | Data Generator | $([ -f "${REPORTS_DIR}/data_generator_${TIMESTAMP}.log" ] && echo "‚úÖ PASSED" || echo "‚ùå FAILED") |
| 1 | Data Cleanup | $([ -f "${REPORTS_DIR}/data_cleanup_${TIMESTAMP}.log" ] && echo "‚úÖ PASSED" || echo "‚ùå FAILED") |
| 2 | Basic Data Flow | $([ -f "${REPORTS_DIR}/basic_data_flow_${TIMESTAMP}.log" ] && echo "‚úÖ PASSED" || echo "‚ùå FAILED") |
| 2 | System Integration | $([ -f "${REPORTS_DIR}/system_integration_${TIMESTAMP}.log" ] && echo "‚úÖ PASSED" || echo "‚ùå FAILED") |
| 2 | Real-time Validation | $([ -f "${REPORTS_DIR}/realtime_validation_${TIMESTAMP}.log" ] && echo "‚úÖ PASSED" || echo "‚ùå FAILED") |
| 3 | Load Testing | $([ -f "${REPORTS_DIR}/load_testing_${TIMESTAMP}.log" ] && echo "‚úÖ PASSED" || echo "‚ùå FAILED") |
| 3 | Throughput Testing | $([ -f "${REPORTS_DIR}/throughput_testing_${TIMESTAMP}.log" ] && echo "‚úÖ PASSED" || echo "‚ùå FAILED") |
| 3 | Stress Testing | $([ -f "${REPORTS_DIR}/stress_testing_${TIMESTAMP}.log" ] && echo "‚úÖ PASSED" || echo "‚ùå FAILED") |
| 4 | Chaos Testing | $([ -f "${REPORTS_DIR}/chaos_testing_${TIMESTAMP}.log" ] && echo "‚úÖ PASSED" || echo "‚ùå FAILED") |
| 4 | Data Consistency | $([ -f "${REPORTS_DIR}/data_consistency_${TIMESTAMP}.log" ] && echo "‚úÖ PASSED" || echo "‚ùå FAILED") |
| 5 | Service Restart | $([ -f "${REPORTS_DIR}/service_restart_${TIMESTAMP}.log" ] && echo "‚úÖ PASSED" || echo "‚ùå FAILED") |
| 5 | High Availability | $([ -f "${REPORTS_DIR}/high_availability_${TIMESTAMP}.log" ] && echo "‚úÖ PASSED" || echo "‚ùå FAILED") |
| 6 | Dashboard Updates | $([ -f "${REPORTS_DIR}/dashboard_updates_${TIMESTAMP}.log" ] && echo "‚úÖ PASSED" || echo "‚ùå FAILED") |
| 6 | Equipment Animations | $([ -f "${REPORTS_DIR}/equipment_animations_${TIMESTAMP}.log" ] && echo "‚úÖ PASSED" || echo "‚ùå FAILED") |
| 7 | Production Lines | $([ -f "${REPORTS_DIR}/production_lines_${TIMESTAMP}.log" ] && echo "‚úÖ PASSED" || echo "‚ùå FAILED") |

## Summary

- **Total Tests:** ${TOTAL_TESTS}
- **Passed:** ${PASSED_TESTS}
- **Failed:** ${FAILED_TESTS}
- **Success Rate:** $(( PASSED_TESTS * 100 / TOTAL_TESTS ))%

## Test Environment

- **Docker Compose:** docker-compose.test.yml
- **Redis:** redis-test:6380
- **InfluxDB:** influxdb-test:8087
- **Test Duration:** $(date -d @$(($(date +%s) - ${TIMESTAMP})) -u +%H:%M:%S)

## Detailed Reports

Detailed logs for each test can be found in the \`${REPORTS_DIR}\` directory:

EOF

# Add links to detailed reports
for log_file in ${REPORTS_DIR}/*_${TIMESTAMP}.log; do
    if [ -f "$log_file" ]; then
        echo "- [$(basename "$log_file")]($log_file)" >> ${REPORT_FILE}
    fi
done

# Cleanup
print_status "INFO" "Cleaning up test infrastructure..."
docker-compose -f docker-compose.test.yml down -v

# Final status
echo -e "\n${BLUE}========================================${NC}"
echo -e "${BLUE}  Test Execution Complete${NC}"
echo -e "${BLUE}========================================${NC}"
echo ""
echo -e "üìä ${GREEN}Total Tests: ${TOTAL_TESTS}${NC}"
echo -e "‚úÖ ${GREEN}Passed: ${PASSED_TESTS}${NC}"
echo -e "‚ùå ${RED}Failed: ${FAILED_TESTS}${NC}"
echo -e "üìà ${BLUE}Success Rate: $(( PASSED_TESTS * 100 / TOTAL_TESTS ))%${NC}"
echo ""
echo -e "üìã ${YELLOW}Comprehensive report: ${REPORT_FILE}${NC}"
echo -e "üìÅ ${YELLOW}Detailed logs: ${REPORTS_DIR}${NC}"

# Exit with appropriate code
if [ ${FAILED_TESTS} -eq 0 ]; then
    print_status "SUCCESS" "All tests passed!"
    exit 0
else
    print_status "FAILED" "Some tests failed. Check the reports for details."
    exit 1
fi